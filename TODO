part 1
download wikidump                                   ............... done
wikiextractor in json mode                          ............... done
read file line by line
parse input json ---- convert input from json to python dictionary ................ done
join title and text with a space in the middle      ............... done
make a list of all title text things                ................ done
do it for all files ..................... done


Part 2
each doc in make list normalize                .................... done
then segment                                   .................... done
take the words add to a dictionary              .................... done
- update the count each time you get a new word         .................... done
counter to the dictionary of words                .................... done

print the top 200 words in their frequencies     .................... done
manually select which of this are stop words
and make them into an array
and delete htis word from the vocbualary dic

convert each doc into a list of term-ids and how many time this term id appears
- assign each word in the dictionary an id
- make a function which takes one tokenised doc and convert it into a list of termid-term frequency pairs


make tf-idf
-Step 1. Convert every paragraph into a dictionary of {term_id: number of times term occurs} .................. done
        - take the sorted segmented list
        - make a function that counts the occurence of an element in the dictionary and put that in a dictionary
        - apply that to the entire sorted segmented list
        - you will get docID-termID-frequency

Step 2: make an array which counts term_id: number of times term occurs in the entire Wikipedia (you already did this to count most common words)
      - change into df - in how many documents a term appeared
      - sorted the segmented
      - deleted the duplicates
      - make df .......................................... done

Step 3 - apply idf to docs .................... done

Step 4 - apply the step 1 function to a given query sentence ................. done
        - write query
        - read query  --- ['ሒሳብ እጅግ በጣም በጣም ጠቃሚና ውበት ያለው የጥናትና የምርምር መስክ ወይም ዘርፍ ነው ።']
        - normalise query ---- --- ['ሂሳብ እጅግ በጣም በጣም ጠቃሚና ውበት ያለው የጥናትና የምርምር መስክ ወይም ዘርፍ ነው ።']
        - segment query --- [['ሂሳብ', 'እጅግ', 'በጣም', 'በጣም', 'ጠቃሚና', 'ውበት', 'ያለው', 'የጥናትና', 'የምርምር', 'መስክ', 'ወይም', 'ዘርፍ', 'ነው', '።']]
        - query to dictionary --- {0: ['ሂሳብ', 'እጅግ', 'በጣም', 'በጣም', 'ጠቃሚና', 'ውበት', 'ያለው', 'የጥናትና', 'የምርምር', 'መስክ', 'ወይም', 'ዘርፍ', 'ነው', '።']}
        - query term to dictionary --- {'ሂሳብ': 1, 'እጅግ': 1, 'በጣም': 2, 'ጠቃሚና': 1, 'ውበት': 1, 'ያለው': 1, 'የጥናትና': 1, 'የምርምር': 1, 'መስክ': 1, 'ወይም': 1, 'ዘርፍ': 1, 'ነው': 1, '።': 1}
        - The sorted segment query --- [['ሂሳብ', 'መስክ', 'በጣም', 'በጣም', 'ነው', 'እጅግ', 'ወይም', 'ውበት', 'ዘርፍ', 'የምርምር', 'የጥናትና', 'ያለው', 'ጠቃሚና', '።']]
        - remove duplicates from the segmented query list --- [['ሂሳብ', 'መስክ', 'በጣም', 'ነው', 'እጅግ', 'ወይም', 'ውበት', 'ዘርፍ', 'የምርምር', 'የጥናትና', 'ያለው', 'ጠቃሚና', '።']]
        - query to dictionary after sorting - term to docID --- {0: ['ሂሳብ', 'መስክ', 'በጣም', 'ነው', 'እጅግ', 'ወይም', 'ውበት', 'ዘርፍ', 'የምርምር', 'የጥናትና', 'ያለው', 'ጠቃሚና', '።']}
        - df - dictionary of {term_id: how many documents a term occured} --- {'ሂሳብ': 1, 'መስክ': 1, 'በጣም': 1, 'ነው': 1, 'እጅግ': 1, 'ወይም': 1, 'ውበት': 1, 'ዘርፍ': 1, 'የምርምር': 1, 'የጥናትና': 1, 'ያለው': 1, 'ጠቃሚና': 1, '።': 1}
        - tf - dictionary of a dictionary{term_id: number of times term occurs} --- {0: {'ሂሳብ': 1, 'መስክ': 1, 'በጣም': 2, 'ነው': 1, 'እጅግ': 1, 'ወይም': 1, 'ውበት': 1, 'ዘርፍ': 1, 'የምርምር': 1, 'የጥናትና': 1, 'ያለው': 1, 'ጠቃሚና': 1, '።': 1}}
        (REMOVED) - idf - apply idf to the docs --- {'ሂሳብ': 0.0, 'መስክ': 0.0, 'በጣም': 0.0, 'ነው': 0.0, 'እጅግ': 0.0, 'ወይም': 0.0, 'ውበት': 0.0, 'ዘርፍ': 0.0, 'የምርምር': 0.0, 'የጥናትና': 0.0, 'ያለው': 0.0, 'ጠቃሚና': 0.0, '።': 0.0}

Step 5 - for each document in wiki, calculate the matching score
Step 6 - rank all documents by step 5 score, return top 10 docs


For the tf idf thingy:

Step 1 - write a function that takes as input tf for 1 query, and idf for 1 document, and calculates the score.

To calculate the score, you go term by term through the query, check if it is in the document, and multiply the tf with the idf to get the score for this term.

Add the score for all the terms to get the full score.
